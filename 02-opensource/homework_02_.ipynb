{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework: Open-Source LLMs\n",
    "\n",
    "## Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the \n",
    "same command as in the lectures:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "What's the version of ollama client? \n",
    "\n",
    "To find out, enter the container and execute `ollama` with the `-v` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "\n",
    "# ollama version is 0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Downloading an LLM \n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b. \n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2 - Below are the content related to gemma:2b in the ollama root directory \n",
    "\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Q3 Below is the response from Ollama gemma 2b\n",
    "\n",
    "\"10 * 10\". What's the answer?\n",
    "\n",
    "The answer to the question \"10 * 10\" is 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use `du -h` for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4 - The size is around 1.7G\n",
    "\n",
    "\n",
    "âžœ /workspaces/llm_zoomcamp (main) $ du -h ollama_files/models\n",
    "1.6G    ollama_files/models/blobs\n",
    "\n",
    "8.0K    ollama_files/models/manifests/registry.ollama.ai/library/gemma\n",
    "\n",
    "12K     ollama_files/models/manifests/registry.ollama.ai/library\n",
    "\n",
    "16K     ollama_files/models/manifests/registry.ollama.ai\n",
    "\n",
    "20K     ollama_files/models/manifests\n",
    "\n",
    "1.6G    ollama_files/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Adding the weights \n",
    "\n",
    "Let's now stop the container and add the weights \n",
    "to a new image\n",
    "\n",
    "For that, let's create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ...\n",
    "```\n",
    "\n",
    "What do you put after `COPY`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5\n",
    "\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ollama_files/models /root/.ollama/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Serving it \n",
    "\n",
    "Let's build it:\n",
    "\n",
    "```bash\n",
    "docker build -t ollama-gemma2b .\n",
    "```\n",
    "\n",
    "And run it:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "```\n",
    "\n",
    "We can connect to it using the OpenAI client\n",
    "\n",
    "Let's test it with the following prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"What's the formula for energy?\"\n",
    "```\n",
    "\n",
    "Also, to make results reproducible, set the `temperature` parameter to 0:\n",
    "\n",
    "```bash\n",
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")\n",
    "```\n",
    "\n",
    "How many completion tokens did you get in response?\n",
    "\n",
    "* 304\n",
    "* 604\n",
    "* 904\n",
    "* 1204\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer - The respnse from gemma 2b \n",
    "\n",
    "\"\"\"\n",
    "Sure, the formula for energy is:\n",
    "\n",
    "**E = 1/2 * m * v^2**\n",
    "\n",
    "Where:\n",
    "\n",
    "* **E** is the energy in joules (J)\n",
    "* **m** is the mass in kilograms (kg)\n",
    "* **v** is the velocity in meters per second (m/s)\n",
    "\n",
    "This formula tells us that the energy of an object is equal to half the product of its mass and the square of its \n",
    "velocity.\n",
    "\"\"\"\n",
    "\n",
    "###  Count of Tokens : 92 (as calculated below) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Counting tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Sure, the formula for energy is:\n",
    "\n",
    "**E = 1/2 * m * v^2**\n",
    "\n",
    "Where:\n",
    "\n",
    "* **E** is the energy in joules (J)\n",
    "* **m** is the mass in kilograms (kg)\n",
    "* **v** is the velocity in meters per second (m/s)\n",
    "\n",
    "This formula tells us that the energy of an object is equal to half the product of its mass and the square of its \n",
    "velocity.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "num_tokens_from_string(text, \"cl100k_base\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
